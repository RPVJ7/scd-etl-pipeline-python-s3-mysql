{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dcb931b-15a8-4a00-a426-bff93b17be11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine , text\n",
    "from sqlalchemy.engine import URL\n",
    "import boto3\n",
    "import json\n",
    "from datetime import date\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ab52573-42c6-49f9-a1bd-3180830f18c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RJ201319\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'namaste-python-ram.s3.amazonaws.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\RJ201319\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'namaste-python-ram.s3.eu-north-1.amazonaws.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\RJ201319\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'namaste-python-ram.s3.eu-north-1.amazonaws.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\RJ201319\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'namaste-python-ram.s3.eu-north-1.amazonaws.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def extract(s3, filename, bucketname):\n",
    "    file = s3.get_object(Bucket=bucketname, Key=filename)\n",
    "    data = file['Body'].read().decode('utf-8')                               # Read the body\n",
    "    json_data = json.loads(data)                                             # Parse as JSON\n",
    "    return json_data\n",
    "\n",
    "def transform(data):\n",
    "    orders_data = []\n",
    "    products_data = []\n",
    "    customers_data = []\n",
    "    for order in data:                                                       #Each order is a separate item in the list which is flattened using for loops\n",
    "\n",
    "        customer_row = {\n",
    "            \"order_id\" : order['order_id'],\n",
    "            \"customer_id\" : order['customer']['customer_id'],\n",
    "            \"name\" : order['customer']['name'],\n",
    "            \"email\" : order['customer']['email'],\n",
    "            \"address\" : order['customer']['address']\n",
    "        }\n",
    "        customers_data.append(customer_row)\n",
    "        \n",
    "        for product in order['products']:                                   #If a order has multiple products, each product is flattened into a separate row\n",
    "            \n",
    "            order_row = {\n",
    "                \"order_id\" : order['order_id'],\n",
    "                \"order_date\" : order['order_date'],\n",
    "                \"total_amount\" : order['total_amount'],\n",
    "                \"customer_id\" : order['customer']['customer_id'],\n",
    "                \"product_id\" : product['product_id'],\n",
    "                \"quantity\" : product['quantity'],\n",
    "            }\n",
    "            orders_data.append(order_row)\n",
    "        \n",
    "            product_row = {\n",
    "               \"order_id\" : order['order_id'],\n",
    "                \"product_id\" : product['product_id'],\n",
    "                \"name\" : product['name'],\n",
    "                \"category\" : product['category'],\n",
    "                \"price\" : product['price']\n",
    "            }\n",
    "            products_data.append(product_row)\n",
    "        \n",
    "    df_orders = pd.DataFrame(orders_data)                                     #creating a orders dataframe which has multiple rows for each order id if there are more than one product\n",
    "\n",
    "    df_products = pd.DataFrame(products_data)\n",
    "    df_products.sort_values(by=['product_id', 'order_id'], ascending=[True, False], inplace=True)\n",
    "    df_products.drop_duplicates('product_id', keep='first', inplace=True)\n",
    "    df_products = df_products.reset_index(drop=True)\n",
    "    df_products.drop(columns='order_id', inplace=True)                        #creating a products dataframe with no duplicates\n",
    "    \n",
    "    df_customers = pd.DataFrame(customers_data)\n",
    "    df_customers.sort_values(by=['customer_id', 'order_id'], ascending=[True, False], inplace=True)\n",
    "    df_customers.drop_duplicates('customer_id', keep='first', inplace=True)\n",
    "    df_customers = df_customers.reset_index(drop=True)\n",
    "    df_customers.drop(columns='order_id', inplace=True)                       #creating a customers dataframe with no duplicates\n",
    "\n",
    "    return df_orders, df_products, df_customers\n",
    "\n",
    "def load(df_orders, df_products, df_customers, engine):\n",
    "    df_customers.to_sql(name=\"customers_stg\", con=engine, index=False, if_exists=\"replace\") #load the customers from recent file into a staging table\n",
    "    df_products.to_sql(name=\"products_stg\", con=engine, index=False, if_exists=\"replace\")   #load the products from recent file into a staging table\n",
    "\n",
    "\n",
    "    #The Python ETL logic inserts new versioned records into the Products dimension table with effective dates and status flags, \n",
    "    #while preserving historical entries, to implement Slowly Changing Dimension Type 2.\n",
    "    \n",
    "    update_query_products = text('''UPDATE products1 p \n",
    "                                    JOIN products_stg s ON p.product_id = s.product_id\n",
    "                                    SET p.end_date = CURDATE() - INTERVAL 1 DAY WHERE p.end_date = '9999-12-31' \n",
    "                                    AND NOT (p.price<=>s.price AND p.name<=>s.name AND p.category<=>s.category)''')\n",
    "    \n",
    "    insert_query_products = text('''INSERT INTO products1 (product_id, name, category, price, start_date, end_date)\n",
    "                                    SELECT s.product_id, s.name, s.category, s.price, CURDATE(), '9999-12-31' FROM products_stg s\n",
    "                                    LEFT JOIN products1 p ON p.product_id = s.product_id AND p.end_date = '9999-12-31'                  \n",
    "                                    WHERE p.product_id IS NULL OR NOT (p.price<=>s.price AND p.name<=>s.name AND p.category<=>s.category)''')\n",
    "\n",
    "    #The Python ETL logic updates the Customers dimension table in place, overwriting old attribute values with the latest source data \n",
    "    #to implement Slowly Changing Dimension Type 1. \n",
    "    \n",
    "    update_query_customers = text('''update customers1 \n",
    "             inner join  customers_stg on customers1.customer_id=customers_stg.customer_id\n",
    "             set customers1.email = customers_stg.email , customers1.name = customers_stg.name\n",
    "             ,customers1.address = customers_stg.address''')\n",
    "    \n",
    "    insert_query_customers = text('''insert into customers1 (customer_id,name,email,address)\n",
    "                     select * from customers_stg\n",
    "                     where customer_id not in (select customer_id from customers1)''')\n",
    "\n",
    "    #The update and insert statements are executed in MySQL\n",
    "    with engine.connect() as conn:\n",
    "        pu = conn.execute(update_query_products)\n",
    "        pi = conn.execute(insert_query_products)\n",
    "        cu = conn.execute(update_query_customers)\n",
    "        ci = conn.execute(insert_query_customers)\n",
    "        conn.commit()\n",
    "\n",
    "    #The dimension tables are read back into a dataframe\n",
    "    df_mysql_products = pd.read_sql(\"select * from products1\", con=engine)\n",
    "    df_mysql_customers = pd.read_sql(\"select * from customers1\", con=engine)\n",
    "\n",
    "    #The fact table is joined with the dimension table to get the surrogate keys and then uploaded into MySQL\n",
    "    df_orders_products = pd.merge(left=df_orders, right=df_mysql_products.loc[df_mysql_products['end_date'] == date(9999, 12, 31), ['product_id', 'product_sk']], how='inner', on='product_id')\n",
    "    df_orders_products_customers = pd.merge(left=df_orders_products, right=df_mysql_customers.loc[df_mysql_customers['end_date'] == date(9999, 12, 31),['customer_id', 'customer_sk']], how='inner', on='customer_id')\n",
    "    columns = ['order_id','order_date','total_amount','customer_sk','product_sk', 'quantity']\n",
    "    df_orders_final = df_orders_products_customers[columns]\n",
    "    df_orders_final.to_sql(name=\"orders1\", con=engine, index=False, if_exists=\"append\")\n",
    "\n",
    "def main():\n",
    "    #Creating a connection with MySQL database where the dimension and fact tables will be updated\n",
    "    engine = create_engine(URL.create(\n",
    "    \"mysql+pymysql\",\n",
    "    username=\"root\",\n",
    "    password=\"#######\",      \n",
    "    host=\"#######\",\n",
    "    port=3306,\n",
    "    database=\"python\",\n",
    "    ))\n",
    "\n",
    "    #creating a connection with Amazon S3 to get the daily order files\n",
    "    s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=\"#########\",\n",
    "    aws_secret_access_key=\"#########\",\n",
    "    verify=False  # <- USE SECURE CA BUNDLE\n",
    "    )\n",
    "    \n",
    "    today = date.today()\n",
    "    formatted = today.strftime(\"%Y%m%d\")\n",
    "    \n",
    "    bucket_name = \"namaste-python-ram\"\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name) #To get list of files and folders within a specific bucket in S3\n",
    "\n",
    "    #Getting the list of JSON files containing the orders for the specific day and processing them one by one in a loop\n",
    "    for obj in response.get(\"Contents\", []):\n",
    "        if obj[\"Key\"].split('.')[-1] == \"json\" and obj['Key'].split('/')[1] == formatted and obj['Key'].split('/')[0] == \"etl_pipeline\":  \n",
    "            data = extract(s3, obj[\"Key\"], bucket_name)               #extracts the data in a file into JSON format\n",
    "            df_orders, df_products, df_customers = transform(data)    #flattens the data in JSON format into fact and dimension tables in dataframe format\n",
    "            load(df_orders, df_products, df_customers, engine)        #loads the data into relevant tables in warehouse in MySQL\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ededb88-7737-4ce1-a269-4c4fb96523be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
